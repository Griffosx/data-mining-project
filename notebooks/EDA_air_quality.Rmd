---
title: "EDA + Modelling of Air Quality Data"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# TODO how to do this properly, env vars?
setwd("~/studies/data-mining-project/notebooks")
INPUT_DIR <- "~/studies/data-mining-project/input"
```

# Exploratory Data Analysis

## Introduction

In this analysis, we will explore the air quality data from various cities in India. The goal is to understand the distribution of air quality metrics, their relationship with weather variables, and build predictive models for the air quality metrics.

### Loading Libraries

```{r}
library(ggplot2)
library(googledrive)
library(dplyr, warn.conflicts = FALSE)
library(readr)
library(magrittr)
library(tidyr, warn.conflicts = FALSE)
library(corrplot)
library(caret)
library(lubridate, warn.conflicts = FALSE)
library(gridExtra, warn.conflicts = FALSE)
library(patchwork)
library(reshape2)
library(zoo)
```

## Loading Data

```{r}
city_name_map <- data.frame(
    air_quality = c("Bengaluru", "Delhi", "Hyderabad", "Jaipur", "Mumbai"),
    weather = c("bengaluru", "delhi", "hyderabad", "jaipur", "bombay")
)

# load the air quality data
data_air_quality <- read_csv(
    file.path(INPUT_DIR, "air_quality", "city_hour.csv"),
    show_col_types = FALSE
)

# filter the data for the cities of interest
data_air_quality <- data_air_quality %>%
    filter(City %in% city_name_map$air_quality) %>%
    filter(Datetime < as.POSIXct("2020-01-01"))

summary(data_air_quality)

# % of NA by column
cat("NA values by column:\n")
sapply(data_air_quality, function(x) sum(is.na(x)) / length(x))
```

Observations:

* There are 12 numeric columns we could use as response variables: PM2.5, PM10, NO, NO2, NOx, NH3, CO, SO2, O3, Benzene, Toluene, Xylene.
* AQI is calculated based on the above columns, so we will not use AQI itself as a response variable.
* All these numeric columns have up to 55% missing values.

## Exploratory Data Analysis

### Distribution of Response Variables

```{r, fig.width=12, fig.height=16}
response_vars <- c("PM2.5", "PM10", "NOx", "NH3", "CO", "SO2", "O3")

# plot the distribution of the response variables
plot_response_variables <- function(response_vars, scale = FALSE) {
    plot_list <- list()
    for (var in response_vars) {
        non_na_data <- data_air_quality %>% filter(!is.na(.data[[var]]))
        n_na <- sum(is.na(data_air_quality[[var]]))

        # color different cities differently
        # and scale the x-axis using a pseudo-log transformation
        p <- ggplot(non_na_data, aes_string(x = var, fill = "City")) +
            geom_histogram(bins = 100)
        if (scale) {
            p <- p + scale_x_continuous(trans=scales::pseudo_log_trans(base = 10))
        } else {
            p <- p + xlim(0, quantile(non_na_data[[var]], 0.99, na.rm = TRUE))
        }
        p <- p + labs(title = var, x = "Value", y = "Count") +
            theme(axis.text = element_text(size = 12)) +
            theme(legend.position = "none") +
            # annotate in the upper right corner
            annotate("text", x = Inf, y = Inf, hjust = 1, vjust = 1, label = paste("NA:", n_na))

        plot_list <- c(plot_list, list(p))
    }
    names(plot_list) <- response_vars

    return(plot_list)
}

# plot the distribution of the response variables
plot_list <- plot_response_variables(response_vars, scale = FALSE)
grid.arrange(grobs = plot_list, ncol = 3)
combined_plot <- wrap_plots(plotlist = plot_list[c("PM2.5", "PM10", "SO2")], ncol = 3) +
    plot_layout(guides = "collect") &
    theme(legend.position = "bottom", legend.text = element_text(size = 12))
ggsave("../report/assets/skewness.png", combined_plot, width = 12, height = 5)

# log-scaled plot
plot_list <- plot_response_variables(response_vars, scale = TRUE)
grid.arrange(grobs = plot_list, ncol = 3)
combined_plot <- wrap_plots(plotlist = plot_list[c("PM2.5", "PM10", "SO2")], ncol = 3) +
    plot_layout(guides = "collect") &
    theme(legend.position = "bottom", legend.text = element_text(size = 12))
ggsave("../report/assets/log-scaled-pollutants.png", combined_plot, width = 12, height = 5)
```

Observations:

* The distributions of the response variables are right-skewed with a long tail.

### Yearly Trend Analysis

Let's plot the monthly boxplots of the response variables for each city.

```{r, fig.width=12, fig.height=20}
plot_monthly_boxplots <- function(response_vars) {
    plot_list <- list()

    # plot boxplots for the response variables by City and Month
    for (var in response_vars) {
        for (city in city_name_map$air_quality) {
            p <- ggplot(
                data_air_quality %>% filter(City == city),
                aes_string(x = "factor(month(Datetime))", y = var)
            ) +
                geom_boxplot(outlier.shape = NA) +
                scale_y_continuous(
                    trans=scales::pseudo_log_trans(base = 10),
                    limits = quantile(data_air_quality[[var]], c(0.05, 0.95), na.rm=TRUE)
                ) +
                theme(axis.text = element_text(size = 12)) +
                labs(x = "Month", y = "Value", title = paste(var, city))

            plot_list <- c(plot_list, list(p))
        }
    }
    names(plot_list) <- paste(rep(response_vars, each = 5), rep(city_name_map$air_quality, length(response_vars)))

    return(plot_list)
}

plot_list <- plot_monthly_boxplots(response_vars)
grid.arrange(grobs = plot_list, ncol = 5)

combined_plot <- wrap_plots(plotlist = plot_list[seq(1, 10)], ncol = 5) +
    plot_layout(guides = "collect") &
    theme(legend.position = "bottom", legend.text = element_text(size = 12))
ggsave("../report/assets/seasonal-trends.png", combined_plot, width = 12, height = 5)
```

Observations:

* The response variables show some seasonal patterns.
* Patterns are similar across cities, but the scale of the values is different.

### Loading Weather Data

```{r}
# load the weather data from multiple CSV files
data_weather <- list()

for (city in city_name_map$weather) {
    file_path <- file.path(INPUT_DIR, "weather", paste0(city, ".csv"))
    this_data <- read_csv(file_path, show_col_types = FALSE)
    this_data$City <- city_name_map$air_quality[city_name_map$weather == city]
    data_weather[[length(data_weather) + 1]] <- this_data

    # cat(paste("Loaded weather data for city", city, "\n"))
}

data_weather <- bind_rows(data_weather)
# remove uvIndex...6 as it contains strange values
data_weather <- data_weather %>%
    select(-uvIndex...6) %>%
    rename(uvIndex = uvIndex...7)

# filter the data
data_weather <- data_weather %>%
    filter(date_time >= as.POSIXct("2015-01-01")) %>%
    filter(date_time < as.POSIXct("2020-01-01"))

summary(data_weather)
```

Observations:

* There are no missing values in the weather data.

### Distribution of Weather Variables

```{r, fig.width=12, fig.height=30}
# plot the distribution of the weather variables
weather_vars <- c(
    "maxtempC", "mintempC", "totalSnow_cm", "sunHour", "uvIndex", "moon_illumination",
    "DewPointC", "FeelsLikeC", "HeatIndexC", "WindChillC", "WindGustKmph", "cloudcover", "humidity",
    "precipMM", "pressure", "tempC", "visibility", "winddirDegree", "windspeedKmph"
)

plot_list <- list()
for (var in weather_vars) {
    col_is_int <- all(as.integer(data_weather[[var]]) == data_weather[[var]])
    n_unique <- length(unique(data_weather[[var]]))

    if (col_is_int && n_unique <= 101) {
        # count the number of observations for each value per City
        barplot_data <- data_weather %>%
            group_by(City, !!sym(var)) %>%
            summarise(n = n(), .groups = "drop") %>%
            ungroup()

        # color different cities differently
        p <- ggplot(barplot_data, aes_string(x = var, y = "n", fill = "City")) +
            geom_bar(stat = "identity") +
            labs(title = var)
    } else {
        # color different cities differently
        p <- ggplot(data_weather, aes_string(x = var, fill = "City")) +
            geom_histogram(bins = min(n_unique, 101)) +
            labs(title = var)
    }

    plot_list <- c(plot_list, list(p))
}

grid.arrange(grobs = plot_list, ncol = 2)
```

## Feature Engineering

The response variables are right-skewed, we will use the log(y + 1) transformation to make the data more normally distributed.

```{r}
# merge the air quality and weather data
data_merged <- inner_join(
    data_air_quality,
    data_weather,
    by = c("City", "Datetime" = "date_time")
)

# create a new variable for the hour of the day
data_merged$hour_of_day <- as.numeric(format(data_merged$Datetime, "%H"))
data_merged$hour_cos <- cos(2 * pi * data_merged$hour_of_day / 24)
data_merged$hour_sin <- sin(2 * pi * data_merged$hour_of_day / 24)
data_merged$hour_2_cos <- cos(2 * pi * data_merged$hour_of_day / 12)
data_merged$hour_2_sin <- sin(2 * pi * data_merged$hour_of_day / 12)
data_merged$hour_of_day <- factor(data_merged$hour_of_day)

# create a new variable for the day of the week
data_merged$day_of_week <- as.numeric(format(data_merged$Datetime, "%u"))
data_merged$day_of_week <- factor(data_merged$day_of_week)

# create a new variable for the month of the year
data_merged$month_of_year <- as.numeric(format(data_merged$Datetime, "%m"))
data_merged$month_of_year <- factor(data_merged$month_of_year)

# convert wind direction to a sin-cos pair
data_merged$winddir_cos <- cos(2 * pi * data_merged$winddirDegree / 360)
data_merged$winddir_sin <- sin(2 * pi * data_merged$winddirDegree / 360)

# calculate wind components using wind speed and direction
data_merged$windspeed_x <- data_merged$windspeedKmph * data_merged$winddir_cos
data_merged$windspeed_y <- data_merged$windspeedKmph * data_merged$winddir_sin

# Create cumulative sum for precipMM
data_merged$precipMM_24h <- ave(
  data_merged$precipMM,
  data_merged$City,
  FUN = function(x) rollapply(x, width = 24, FUN = sum, align = "right", fill = NA, na.rm = TRUE)
)

# Create cumulative sum for windspeedKmph
data_merged$windspeedKmph_24h <- ave(
  data_merged$windspeedKmph,
  data_merged$City,
  FUN = function(x) rollapply(x, width = 24, FUN = sum, align = "right", fill = NA, na.rm = TRUE)
)

# convert the City variable to a factor
data_merged$City <- factor(data_merged$City)

# log(y + 1) transformation for the response variables
data_merged[response_vars] <- log(data_merged[response_vars] + 1)
```

Let's scale the numeric features to have zero mean and unit variance.

```{r}
# determine the numeric features automatically
numeric_vars <- sapply(data_merged, is.numeric)

# remove the response variables from the numeric features
numeric_vars <- numeric_vars & !names(data_merged) %in% (data_air_quality %>% select(-City, -Datetime) %>% colnames())

# scale the numeric variables
data_merged[numeric_vars] <- scale(data_merged[numeric_vars])
```

## Correlation Analysis

### Feature-Feature Correlation

```{r}
weather_vars <- c(
    "maxtempC", "mintempC", "totalSnow_cm", "sunHour", "uvIndex", "moon_illumination",
    "DewPointC", "FeelsLikeC", "HeatIndexC",
    "WindChillC", "WindGustKmph", "cloudcover", "humidity", "precipMM", "pressure",
    "tempC", "visibility", "winddirDegree", "windspeedKmph"
    # "winddir_cos", "winddir_sin", "windspeed_x", "windspeed_y",
    # "precipMM_24h",
    # "windspeedKmph_24h"
)

# calculate the correlation matrix
correlation_matrix <- cor(
    data_merged[, weather_vars],
    use = "pairwise.complete.obs"
)

# reorder the correlation matrix
reordered_indices <- hclust(as.dist(1 - abs(na.fill(correlation_matrix, 0))))$order
correlation_matrix_reordered <- correlation_matrix[reordered_indices, reordered_indices]

correlation_long <- melt(correlation_matrix_reordered)

# plot the correlation matrix
p <- ggplot(correlation_long, aes(x = Var1, y = Var2, fill = na.fill(value, 0))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "blue", mid = "white", high = "red", 
        midpoint = 0, limit = c(-1, 1), space = "Lab",
        name = "Correlation"
    ) +
    geom_text(aes(label = na.fill(as.character(round(value, 2)), "NA")), size = 3.5) +
    theme_minimal() +
    theme(
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text = element_text(size = 12)
    ) +
    labs(
        x = "Variables",
        y = "Variables",
    )
p

ggsave("../report/assets/feature-correlation-matrix.png", p, width = 10, height = 8)
```

Observations:

* The correlation matrix shows that some weather variables are highly correlated with each other.
* We shall remove some of these variables to avoid multicollinearity.
* For example, we can remove the following variables:
    * FeelsLikeC, HeatIndexC, WindChillC, minTempC, maxTempC (highly correlated with tempC, and derived from a combination of temperature, humidity, and wind speed)
    * uvIndex...6 (highly correlated with uvIndex...7)

### Feature-Response Correlation

```{r}
feature_vars <- c(
    "sunHour", "uvIndex", "moon_illumination",
    "DewPointC", "cloudcover", "humidity",
    "precipMM", "pressure", "tempC", "visibility", "winddirDegree", "windspeedKmph",
    "maxtempC", "mintempC"  # TODO: remove highly correlated features
    # "winddir_cos", "winddir_sin", "windspeed_x", "windspeed_y"
    # "precipMM_24h", "windspeedKmph_24h",
)

# calculate the correlation matrix
correlation_matrix <- cor(
    data_merged[, feature_vars],
    data_merged[, response_vars],
    use = "pairwise.complete.obs"
)

# reorder the correlation matrix
correlation_long <- melt(correlation_matrix, na.rm = TRUE)

# plot the correlation matrix
p <- ggplot(correlation_long, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "blue", mid = "white", high = "red", 
        midpoint = 0, limit = c(-1, 1), space = "Lab",
        name = "Correlation"
    ) +
    geom_text(aes(label = round(value, 2)), size = 4) +
    theme_minimal() +
    theme(
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.text = element_text(size = 12)
    ) +
    labs(
        x = "Variables",
        y = "Variables",
    )
p

ggsave("../report/assets/feature-response-correlation.png", p, width = 10, height = 6)
```

Observations:

* The correlation matrix shows that the weather variables are weakly correlated with the response variables.
* However, we only used a linear correlation measure, there might be non-linear relationships.

## Building a Model

Let's build a linear regression model for each response variable using the weather variables as predictors. We will check for multicollinearity and evaluate the model.

Since the data is from multiple cities, we will build separate models for each city.

```{r, fig.width=12, fig.height=16}
feature_vars <- c(
    "sunHour", "uvIndex...7", "moon_illumination", "DewPointC", "WindGustKmph", "cloudcover",
    "humidity", "precipMM", "pressure", "tempC", "windspeedKmph", "day_of_week",
    "month_of_year", "hour_cos", "hour_sin",
    "hour_2_cos", "hour_2_sin",
    "winddir_cos", "winddir_sin",
    "precipMM_cumsum_24h",
    "windspeed_x", "windspeed_y"
)

feature_importance <- list()
stats_values <- list()

for (city in city_name_map$air_quality) {
    cat(paste("\nFitting models for", city, "\n"))

    data_city <- data_merged %>%
        filter(City == city)

    for (var in response_vars) {
        data_train <- data_city %>%
            filter(Datetime < as.POSIXct("2019-01-01")) %>%
            select(c(var, feature_vars)) %>%
            na.omit()
        data_test <- data_city %>%
            filter(Datetime >= as.POSIXct("2019-01-01")) %>%
            select(c(var, feature_vars)) %>%
            na.omit()

        if (nrow(data_train) == 0) {
            cat(paste("Skipping", var, "for", city, "due to all NA values\n"))
            next
        }
        if (length(unique(data_train$month_of_year)) != 12) {
            cat(paste("Skipping", var, "for", city, "due to missing months\n"))
            next
        }

        model <- lm(
            data_train[[var]] ~ .,
            data = data_train[, feature_vars]
        )

        # add the model to the list of feature importance
        feature_importance_row <- data.frame(
            feature = rownames(summary(model)$coefficients)[-1],
            estimate = summary(model)$coefficients[-1, 1],
            response_var = var,
            city = city
        )
        rownames(feature_importance_row) <- NULL
        feature_importance <- c(feature_importance, list(feature_importance_row))

        test_predictions <- predict(model, data_test[, feature_vars])

        stats_values <- c(
            stats_values,
            list(
                data.frame(
                    city = city,
                    var = var,
                    r = cor(data_test[[var]], test_predictions),
                    r2 = cor(data_test[[var]], test_predictions)^2,
                    rmse = sqrt(mean((data_test[[var]] - test_predictions)^2))
                )
            )
        )

        # evaluate the model
        cat(paste(city, var, ":", round(summary(model)$r.squared, 3), "\n"))
    }
}

stats_values <- bind_rows(stats_values)
cat("Mean stats by response variable:\n")
stats_values %>%
    group_by(var) %>%
    summarise(r = mean(r), r2 = mean(r2), rmse = mean(rmse))

print(mean(stats_values$r2))

feature_importance <- bind_rows(feature_importance)

# plot the feature importance
ggplot(feature_importance, aes(x = estimate, y = feature, fill = city)) +
    geom_bar(stat = "identity") +
    facet_wrap(~response_var, scales = "free") +
    labs(title = "Feature Importance")
```

<!-- ## Multicollinearity -->

```{r}
# vif_rows <- list()
# for (var in response_vars) {
#     cat(paste("Checking for multicollinearity for", var, "\n"))

#     # calculate the VIF for each predictor variable
#     model <- lm(data_merged[[var]] ~ ., data = data_merged[, c(var, feature_vars)])
#     vif_values <- car::vif(model)

#     vif_row <- as.data.frame(t(vif_values[-1, 3]))
#     vif_row[["response_var"]] <- var

#     vif_rows <- c(vif_rows, list(vif_row))
# }

# vif_data <- bind_rows(vif_rows)
# print(vif_data)
```
